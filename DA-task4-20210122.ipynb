{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的package\n",
    "import seaborn as sns #用于画图\n",
    "from bs4 import BeautifulSoup #用于爬取arxiv的数据\n",
    "import re #用于正则表达式，匹配字符串的模式\n",
    "import requests #用于网络连接，发送网络请求，使用域名获取对应信息\n",
    "import json #读取数据，我们的数据为json格式的\n",
    "import pandas as pd #数据处理，数据分析\n",
    "import matplotlib.pyplot as plt #画图工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArxivFile(path, columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n",
    "       'report-no', 'categories', 'license', 'abstract', 'versions',\n",
    "       'update_date', 'authors_parsed'], count=None):\n",
    "    '''\n",
    "    定义读取文件的函数\n",
    "        path: 文件路径\n",
    "        columns: 需要选择的列\n",
    "        count: 读取行数\n",
    "    '''\n",
    "    \n",
    "    data  = []\n",
    "    with open(path, 'r') as f: \n",
    "        for idx, line in enumerate(f): \n",
    "            if idx == count:\n",
    "                break\n",
    "                \n",
    "            d = json.loads(line)\n",
    "            d = {col : d[col] for col in columns}\n",
    "            data.append(d)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "data = readArxivFile('arxiv-metadata-oai-2019.json', \n",
    "                     ['id', 'title', 'categories', 'abstract'],\n",
    "                    200000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                              title categories  \\\n",
      "0  0704.0297  Remnant evolution after a carbon-oxygen white ...   astro-ph   \n",
      "1  0704.0342  Cofibrations in the Category of Frolicher Spac...    math.AT   \n",
      "2  0704.0360  Torsional oscillations of longitudinally inhom...   astro-ph   \n",
      "3  0704.0525  On the Energy-Momentum Problem in Static Einst...      gr-qc   \n",
      "4  0704.0535  The Formation of Globular Cluster Systems in M...   astro-ph   \n",
      "\n",
      "                                            abstract  \n",
      "0    We systematically explore the evolution of t...  \n",
      "1    Cofibrations are defined in the category of ...  \n",
      "2    We explore the effect of an inhomogeneous ma...  \n",
      "3    This paper has been removed by arXiv adminis...  \n",
      "4    The most massive elliptical galaxies show a ...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将标题和摘要拼接一起完成分类。\n",
    "data['text'] = data['title'] + data['abstract']\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.replace('\\n',' '))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data = data.drop(['abstract', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 备注函数\n",
    "Python lower() 方法转换字符串中所有大写字符为小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [astro-ph]\n",
      "1     [math.AT]\n",
      "2    [astro-ph]\n",
      "3       [gr-qc]\n",
      "4    [astro-ph]\n",
      "5     [nucl-ex]\n",
      "6    [quant-ph]\n",
      "7     [math.DG]\n",
      "8      [hep-ex]\n",
      "9    [astro-ph]\n",
      "Name: categories, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 多个类别，包含子分类\n",
    "data['categories'] = data['categories'].apply(lambda x : x.split(' '))\n",
    "print(data['categories'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [astro-ph]\n",
      "1        [math]\n",
      "2    [astro-ph]\n",
      "3       [gr-qc]\n",
      "4    [astro-ph]\n",
      "5     [nucl-ex]\n",
      "6    [quant-ph]\n",
      "7        [math]\n",
      "8      [hep-ex]\n",
      "9    [astro-ph]\n",
      "Name: categories_big, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 单个类别，不包含子分类 因为这里的.后代表子分类\n",
    "data['categories_big'] = data['categories'].apply(lambda x : [xx.split('.')[0] for xx in x])\n",
    "print(data['categories_big'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行编码\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "data_label = mlb.fit_transform(data['categories_big'].iloc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路1：\n",
    "思路1使用TFIDF提取特征，限制最多4000个单词\n",
    "思路1：TF-IDF+机器学习分类器\n",
    "直接使用TF-IDF对文本提取特征，使用分类器进行分类，分类器的选择上可以使用SVM、LR、XGboost等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=4000)\n",
    "data_tfidf = vectorizer.fit_transform(data['text'].iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_tfidf, data_label,\n",
    "                                                 test_size = 0.2,random_state = 1)\n",
    "\n",
    "# 构建多标签分类模型\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultiOutputClassifier(MultinomialNB()).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.91      0.85      0.88      3625\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.77      0.76      0.77      3801\n",
      "           9       0.84      0.89      0.86     10715\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00       186\n",
      "          12       0.44      0.41      0.42      1621\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.75      0.59      0.66      1096\n",
      "          15       0.61      0.80      0.69      1078\n",
      "          16       0.90      0.19      0.32       242\n",
      "          17       0.53      0.67      0.59      1451\n",
      "          18       0.71      0.54      0.62      1400\n",
      "          19       0.88      0.84      0.86     10243\n",
      "          20       0.40      0.09      0.15       934\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.88      0.03      0.07       414\n",
      "          23       0.48      0.65      0.55       517\n",
      "          24       0.37      0.33      0.35       539\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.60      0.42      0.49      3891\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.82      0.08      0.15       676\n",
      "          29       0.86      0.12      0.21       297\n",
      "          30       0.80      0.40      0.53      1714\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.56      0.65      0.60      3398\n",
      "          33       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.76      0.70      0.72     47851\n",
      "   macro avg       0.39      0.27      0.29     47851\n",
      "weighted avg       0.75      0.70      0.71     47851\n",
      " samples avg       0.74      0.76      0.72     47851\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caroline/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/caroline/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/caroline/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路2：FastText\n",
    "FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建分类器\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'].iloc[:100000], \n",
    "                                                    data_label[:100000],\n",
    "                                                 test_size = 0.95,random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-50cd44204cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "max_features= 500\n",
    "max_len= 150\n",
    "embed_size=100\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokens = Tokenizer(num_words = max_features)\n",
    "tokens.fit_on_texts(list(data['text'].iloc[:100000]))\n",
    "\n",
    "y_train = data_label[:100000]\n",
    "x_sub_train = tokens.texts_to_sequences(data['text'].iloc[:100000])\n",
    "x_sub_train = sequence.pad_sequences(x_sub_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思路3：WordVec+深度学习分类器\n",
    "WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRnn或者BiLSTM。\n",
    "\n",
    "思路4：Bert词向量\n",
    "Bert是高配款的词向量，具有强大的建模学习能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
